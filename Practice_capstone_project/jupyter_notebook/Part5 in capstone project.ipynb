{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [step0](#step0): import necessary packages\n",
    "* [step1](#step1): import dataset part4_dataset.pickle as part5_dataset\n",
    "* [step2](#step2): combine Positive_Review and Negative_Review into one text column\n",
    "* [step3](#step3): replace the punctuation in the string `combined_review`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno # module for missing value visualization\n",
    "from scipy import stats # implement box-cox transformation\n",
    "from math import ceil\n",
    "from string import strip # Return a copy of the string with leading and trailing characters removed\n",
    "from sklearn.utils import shuffle # shuffling the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step1\"></a>\n",
    "#### step1: import dataset part4_dataset.pickle as part5_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part5_dataset = pd.read_pickle(\"part4_dataset.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step2\"></a>\n",
    "#### step2: combine Positive_Review and Negative_Review into one text column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rooms are nice but for elderly a bit difficult as most rooms are two story with narrow steps So ask for single level Inside the rooms are very very basic just tea coffee and boiler and no bar empty fridge Location was good and staff were ok It is cute hotel the breakfast range is nice Will go back'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# combine Positive_Review and Negative_Review into one text column\n",
    "# strip the whitespace at both ends\n",
    "part5_dataset.Negative_Review = part5_dataset.Negative_Review.apply(lambda x: strip(x))\n",
    "part5_dataset.Positive_Review = part5_dataset.Positive_Review.apply(lambda x: strip(x))\n",
    "\n",
    "# combine the two text column\n",
    "part5_dataset[\"combined_review\"] = part5_dataset[[\"Negative_Review\",\"Positive_Review\"]].apply(lambda x: \" \".join(x), axis=1)\n",
    "\n",
    "# have a look at the result\n",
    "display(part5_dataset[[\"combined_review\",\"Negative_Review\",\"Positive_Review\"]].iloc[2,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step3\"></a>\n",
    "#### step3: replace the punctuation in the string `combined_review`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# replace the punctuation in the string \"combined_review\" except alphanumeric character and white-space\n",
    "part5_dataset[\"combined_review\"] = part5_dataset[\"combined_review\"].str.replace(\"[^\\w\\s]\",\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step4\"></a>\n",
    "#### step4: save the output as `part5_dataset.pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part5_dataset.to_pickle(\"part5_dataset.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step3\"></a>\n",
    "#### step3: shuffle and sampling 50% of the dataset\n",
    "1. Use the first 50% of dataset as training and validation dataset for sentiment analysis.\n",
    "2. Also for the simplicity of analysis, I will also use the training dataset as the training text for LDA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# shuffle and sampling 50% of the dataset\n",
    "shuffled_data = shuffle(part5_dataset, random_state=20)\n",
    "\n",
    "# separate target variable out\n",
    "target_variable = shuffled_data.review_sentiment\n",
    "target_variable = target_variable.astype(\"category\")\n",
    "\n",
    "# just sample 50% of the whole dataset - use train_test_split() to achieve same result\n",
    "X_first50, X_remaining50, y_first50, y_remaining50 = train_test_split(shuffled_data, target_variable, \\\n",
    "                                                                      test_size = 0.5, stratify = target_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step4\"></a>\n",
    "#### step4: create a bag of words solely for LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a class for lemmatizer\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function to show up the topic words\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: [extended slices](https://docs.python.org/2.3/whatsnew/section-slices.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build up a bag of words for LDA model\n",
    "n_features = 5000\n",
    "\n",
    "lda_tfidf_vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(), \\\n",
    "                                       max_df=0.25, min_df=2, \\\n",
    "                                       max_features=n_features, \\\n",
    "                                       stop_words=\"english\")\n",
    "\n",
    "# fit and transform data\n",
    "lda_tfidf = lda_tfidf_vectorizer.fit_transform(X_first50[\"combined_review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the feature names in the bag of words\n",
    "lda_tfidf_feature_names = lda_tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: t u time check like night didn reception positive day stay service did breakfast got booking star bar stayed booked\n",
      "Topic #1: great friendly excellent helpful breakfast good clean comfortable bed nice lovely perfect amazing service fantastic stay comfy view really bar\n",
      "Topic #2: close station good walk value city metro money near parking restaurant easy minute great tube far breakfast nice walking central\n",
      "Topic #3: positive small bed bathroom breakfast shower good coffee air hot water window noisy comfortable poor tea nice bit clean little\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# build up a LDA model\n",
    "n_topics = 4\n",
    "n_top_words = 20\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "# fit in the data\n",
    "lda.fit(lda_tfidf)\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "lda_tfidf_feature_names = lda_tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, lda_tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the probabilities of topics for each document(row)\n",
    "doc_topic_distribution = lda.transform(lda_tfidf) # already being normalized and will sum up to 1\n",
    "\n",
    "# receive the index for topic with maximum probability\n",
    "topic_for_doc = doc_topic_distribution.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: \n",
    "1. [LDA probability for document](https://github.com/scikit-learn/scikit-learn/issues/6320)\n",
    "2. [Use .transform for topic probatility](https://stackoverflow.com/questions/45150329/how-to-get-the-topics-probability-of-a-specific-document-using-scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Actually everything was perfect with Pullman Hotel but after we checked out on Dec 23rd around 10AM they sent us email at 7 40PM told us that the housekeeper couldn t find the hairdryer in our room and they wanted to charge us 80 We didn t take the hairdryer and we explained to them where we put the hairdryer back to it s place They replied that they will check again to the housekeeping and until now still no news from them I hope they already find the hairdryer and I still wait for my credit card bill to make sure there is no charge from Pullman Hotel The Hotel is very near from Eiffel Tower 2mins walking distance The room was spacious and very clean They upgraded our room from superior king room eiffel tower view to balcony eiffel tower view because that was our honeymoon trip Thank you so much Pullman'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_first50[\"combined_review\"].iloc[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 3, 0])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_for_doc[16:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the probabilities of topics for each document(row) - the original dataset\n",
    "original_doc = lda_tfidf_vectorizer.transform(part5_dataset[\"combined_review\"]) # transform data into bag of words\n",
    "\n",
    "original_doc_topic = lda.transform(original_doc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
