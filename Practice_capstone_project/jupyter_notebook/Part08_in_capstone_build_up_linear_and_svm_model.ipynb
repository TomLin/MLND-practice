{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of Steps\n",
    "* [step0](#step0): import necessary packages\n",
    "* [step1](#step1): import `dataset X_remaining50.pickle` as `X_remaining50`\n",
    "* [step2](#step2): select the relevant predictors and target variable\n",
    "* [step3](#step3): shuffle and sampling  the remaining dataset\n",
    "* [step4](#step4): create Lasso model as benchmark model - using default parameter\n",
    "* [step5](#step5): create Lasso model as benchmark model - using grid-search\n",
    "* [step6](#step6): optimized Lasso model's performance on testing dataset \n",
    "* [step7](#step7): insights of coefficients from Lasso model\n",
    "* [step8](#step8): create comparing model support vector regression - using default parameter\n",
    "* [step9](#step9): create comparing model support vector regression - using grid-search\n",
    "* [step10](#step10): optimized SVR model's performance on testing dataset\n",
    "* [step11](#step11): choose better model and predict the rating for the testing dataset\n",
    "* [step12](#step12): save the output dataset for later use\n",
    "* [step13](#step13): appendix - increase the sample number in training dataset and re-fit Lasso model again\n",
    "* [step14](#step14): appendix - try `DecisionTreeRegressor` for rate prediction model\n",
    "* [step15](#step15): appendix - try `AdaBoosting` for rate prediction model\n",
    "* [step16](#step16): appendix - try `multi-layer perceptron` for rate prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno # module for missing value visualization\n",
    "from scipy import stats # implement box-cox transformation\n",
    "from math import ceil\n",
    "from sklearn.utils import shuffle # shuffling the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB # for sentiment analysis benchmark model\n",
    "from sklearn.model_selection import cross_val_score # cross validation score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from scipy.stats import uniform\n",
    "from numpy import flatnonzero # return the index for nonzero value\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None # show up all column values in display\n",
    "\n",
    "# suppress warning\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# suppress scientific notation\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step1\"></a>\n",
    "## step1: import dataset `X_remaining50.pickle` as `X_remaining50`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_remaining50 = pd.read_pickle(\"X_remaining50.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step2\"></a>\n",
    "## step2: select the relevant predictors and target variable\n",
    "1. the target variable is `transformed_score`.\n",
    "2. based on the correlation matrix showed up in part3 and part4, following are some explanatory features worth for trying. \n",
    "    - a. `mlp_predict_review_sentiment`\n",
    "    - b. `transformed_review_total_negative_word_counts` \n",
    "    - c. `transformed_review_total_positive_word_counts`\n",
    "    - d. `transformed_average_score`\n",
    "    - e. `quarter_transformed_score`\n",
    "    - f. `quarter_previous_transformed_score`\n",
    "    - g. They all have higher correlation with `transformed_score`.\n",
    "3. in order to apply pd.get_dummies(), I first replace the value 0/1 in `mlp_predict_review_sentiment` to string.\n",
    "4. drop out the NA in the rows, so that I won't face bugs when applying MinMaxScaler()\n",
    "5. set up the dummies for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select the relevant target variable and predictors\n",
    "cols = [\"transformed_score\",\n",
    "        \"mlp_predict_review_sentiment\",\n",
    "        \"transformed_review_total_negative_word_counts\",\n",
    "        \"transformed_review_total_positive_word_counts\",\n",
    "        \"transformed_average_score\",\n",
    "        \"quarter_transformed_score\",\n",
    "        \"quarter_previous_transformed_score\"]\n",
    "\n",
    "X_remaining_raw = X_remaining50[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map 0/1 in mlp_predict_review_sentiment to negative/positive sentiment\n",
    "value_replace = {0:\"negative\",\n",
    "                 1:\"positive\"}\n",
    "\n",
    "X_remaining_raw[\"mlp_predict_review_sentiment\"] = X_remaining_raw[\"mlp_predict_review_sentiment\"].map(value_replace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop out row contains NA\n",
    "X_remaining_raw.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert categorical feature into dummies\n",
    "X_remaining_raw = pd.get_dummies(X_remaining_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step3\"></a>\n",
    "## step3: shuffle and sampling  the remaining dataset\n",
    "1. In previous experiments, if the number of samples goes beyond 10000 in Support Vector Machine, the training time will be super long. Thus, in order to avoid long training hours, I will universally use only **5%** (that is around *11728* samples) as training dataset for both Lasso and SVM model.\n",
    "2. The 5% dataset will serve as training and validation dataset.\n",
    "3. Use the remaining 95% of dataset as testing dataset for rating prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate target variable out - transformed_score\n",
    "target_variable = X_remaining_raw.transformed_score\n",
    "\n",
    "# drop out the target variable in X dataset\n",
    "X_remaining_sub = X_remaining_raw.drop([\"transformed_score\"], axis=1)\n",
    "\n",
    "# use the remaining 95% as the testing dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_remaining_sub, target_variable,\n",
    "                                                    test_size = 0.95, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step4\"></a>\n",
    "## step4: create Lasso model as benchmark model - using default parameter\n",
    "Default parameter for Lasso model:\n",
    "* alpha = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average cross-validation R^2 score of default Lasso model: 0.3868\n"
     ]
    }
   ],
   "source": [
    "# create a Lasso model with default argument\n",
    "lasso_default = Lasso()\n",
    "\n",
    "# define scoring function\n",
    "scorer = make_scorer(r2_score, greater_is_better=True)\n",
    "\n",
    "# compute cross-validation score\n",
    "scores = cross_val_score(lasso_default, X_train, y_train, cv=5)\n",
    "\n",
    "# print out the average score\n",
    "print(\"average cross-validation R^2 score of default Lasso model: {:.4f}\".format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step5\"></a>\n",
    "## step5: create Lasso model as benchmark model - using grid-search\n",
    "There are two parameters being fine tuned in grid-search. One is **alpha**, when alpha goes down, the model will become more complicated, hence overfitting. In the meantime, **max_iter** will need to go up responding to smaller alpha.\n",
    "\n",
    "Comparing the **R^2** between the default parameter and the optimized parameter:\n",
    "1. before: 0.3868\n",
    "2. after: 0.3870\n",
    "3. the parameters being chosen by grid-search are:\n",
    "    - alpha: 0.001\n",
    "    - max_iter: 10000\n",
    "\n",
    "**BRIEF RESULT:**\n",
    "1. As we can tell, the `GridSearchCV` chose to have smaller alpha and larger max_iter, comparing to the default parameters, implying that default model is still a bit of underfitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best R^2 of all model parameters' combination on Lasso model: 0.3870\n"
     ]
    }
   ],
   "source": [
    "# create a Lasso model\n",
    "lasso = Lasso()\n",
    "\n",
    "# set up the parameter range for grid-search\n",
    "param_grid = {\"alpha\":[80,50,20,15,10,5,3,2,1,0.5,0.1,0.001],\n",
    "              \"max_iter\":[10000,5000,1000,500,100]}\n",
    "\n",
    "scorer = make_scorer(r2_score, greater_is_better=True)\n",
    "\n",
    "grid = GridSearchCV(lasso, param_grid=param_grid, scoring=scorer, cv=5)\n",
    "\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "print(\"the best R^2 of all model parameters' combination on Lasso model: {:.4f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the parameter setting of optimized Lasso model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=10000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the argument setting for best estimator\n",
    "print(\"the parameter setting of optimized Lasso model\")\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step6\"></a>\n",
    "## step6: optimized Lasso model's performance on testing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the R^2 of the optimized Lasso model on testing dataset:\n",
      "0.3899\n"
     ]
    }
   ],
   "source": [
    "# the estimator's performance on the testing dataset\n",
    "print(\"the R^2 of the optimized Lasso model on testing dataset:\")\n",
    "print(\"{:.4f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step7\"></a>\n",
    "## step7: insights of coefficients from Lasso model\n",
    "Because Lasso model can suppress the coefficients of insignificant explanatory variables to 0, it's a good way to inspect the what explanatory variables are actually important in predicting the target variable - `transformed_score`.\n",
    "\n",
    "**NOTICE:** Owing to the target variable and explanatory variables are being box-cox re-scaled, we can only interpret the importance of variables based on coefficients. The absolute value of coefficient doesn't represent its effect on the original target variable.\n",
    "\n",
    "**BRIEF RESULT:**\n",
    "1. From the coefficient of each feature, we can tell that **`transformed_review_total_positive_word_counts`** is significantly correlated to higher scores.\n",
    "2. On the contrary, **`transformed_review_total_negative_word_counts`** and **`mlp_predict_review_sentiment_negative`** are significantly correlated to lower scores. \n",
    "3. One interesting discovery is that the `mlp_predict_review_sentiment_negative` seems quite useful in identifying low scores, proving its value of text sentiment model. Nonetheless, it's not so powerful in identifying high score instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out the nonzero coefficients\n",
    "nonzero_index = flatnonzero(grid.best_estimator_.coef_) # return the index for nonzero coefficients\n",
    "nonzero_feature_name = X_train.columns[nonzero_index] # index out the feature name\n",
    "nonzero_coef = grid.best_estimator_.coef_[nonzero_index] # index out the coefficient's value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformed_review_total_positive_word_counts</td>\n",
       "      <td>16.342751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quarter_transformed_score</td>\n",
       "      <td>0.684411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transformed_average_score</td>\n",
       "      <td>0.019337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quarter_previous_transformed_score</td>\n",
       "      <td>0.009739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mlp_predict_review_sentiment_positive</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>transformed_review_total_negative_word_counts</td>\n",
       "      <td>-17.836231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>mlp_predict_review_sentiment_negative</td>\n",
       "      <td>-40.343272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         feature      value\n",
       "0  transformed_review_total_positive_word_counts  16.342751\n",
       "1                      quarter_transformed_score   0.684411\n",
       "2                      transformed_average_score   0.019337\n",
       "3             quarter_previous_transformed_score   0.009739\n",
       "4          mlp_predict_review_sentiment_positive   0.000000\n",
       "5  transformed_review_total_negative_word_counts -17.836231\n",
       "6          mlp_predict_review_sentiment_negative -40.343272"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the coefficients as sorted dataframe\n",
    "value = nonzero_coef[nonzero_coef.argsort()[::-1]]\n",
    "value = np.round(value, 6) # avoid scientific notation\n",
    "feature = nonzero_feature_name[nonzero_coef.argsort()[::-1]]\n",
    "lasso_coefficient = pd.DataFrame({\"feature\":feature, \"value\":value})\n",
    "display(lasso_coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step8\"></a>\n",
    "## step8: create comparing model support vector regression - using default parameter\n",
    "Default parameter for SVR:\n",
    "* C = 1\n",
    "* gamma = 1/n_features ('auto')\n",
    "\n",
    "**SPECIAL NOTICE**:\n",
    "1. When training Support Vector Machine, it requires to have similar scale on all features. For I have dummy variables(0/1), I will use MinMaxScaler() to scale all numeric features into range 0/1 as well. \n",
    "2. In the following model evaluation, for the proper use of train and validation dataset in cross-validation, it's better to create a pipeline for it.\n",
    "    - For each fold of cross-validation, pipeline enables to use the training data of current fold only and create scaler based on it. It avoids data information leakage to validation dataset.\n",
    "3. In training Support Vector Machine, the samples of dataset is recommended not to go beyond 100,000 rows, the time it takes for training the model goes exponentially. In my model training process, I will choose samples not to go beyond 10,000 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average cross-validation R^2 score of default SVR model: 0.27\n"
     ]
    }
   ],
   "source": [
    "# create a pipeline with default model parameter\n",
    "svr_pipe_default = Pipeline([(\"scaler\", MinMaxScaler()),(\"svr\", SVR(C=1, gamma = 'auto'))])\n",
    "\n",
    "# define scoring function\n",
    "scorer = make_scorer(r2_score, greater_is_better=True)\n",
    "\n",
    "# compute cross-validation score\n",
    "scores = cross_val_score(svr_pipe_default, X_train, y_train, cv=5)\n",
    "\n",
    "# print out the average score\n",
    "print(\"average cross-validation R^2 score of default SVR model: {:.2f}\".format(scores.mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step9\"></a>\n",
    "## step9: create comparing model support vector regression - using grid-search\n",
    "1. In order to speed up the model training process, I will try to use `RandomizedSearchCV` this time, instead of `GridSearchCV`. The `RandomizedSearchCV` will only select some combinations of parameters, hence reducing training time.\n",
    "2. `RandomizedSearchCV` uses distribution instead of hard-code number for specifying parameters.\n",
    "3. In SVR model, two parameters are being fine tuned. One is **C**, which is similar to the L1 regularization in Lasso model. When C goes up, the model will move toward overfitting. The other is **gamma**, which means how far each sample's influence is. If gamma goes up, every sample will have shorter range of influence, hence the model will move toward overfitting.\n",
    "\n",
    "Comparing the **R^2** between the default parameter and the optimized parameter:\n",
    "1. before: 0.27\n",
    "2. after: 0.41\n",
    "3. the parameters being chosen by randomized-grid-search are:\n",
    "    - C: 8.91\n",
    "    - gamma: 8.15\n",
    "\n",
    "**BRIEF RESULT:**\n",
    "1. As we can tell, the `RandomizedSearchCV` chose to have larger C and gamma, comparing to the default parameters, implying that default model is still a bit of underfitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best R^2 of all model parameters' combination on SVR model: 0.41\n"
     ]
    }
   ],
   "source": [
    "# create a randomized pipeline\n",
    "svr_pipe = Pipeline([(\"scaler\", MinMaxScaler()),(\"svr\", SVR())])\n",
    "\n",
    "\n",
    "# set up the parameter range for grid-search\n",
    "param_grid = {\"svr__C\":uniform(0,10), # use distributions insted (only applicable in randomized grid search)\n",
    "              \"svr__gamma\":uniform(0,10)}\n",
    "\n",
    "scorer = make_scorer(r2_score, greater_is_better=True)\n",
    "\n",
    "random_grid = RandomizedSearchCV(svr_pipe, param_distributions=param_grid, # use param_distributions\n",
    "                                 scoring=scorer, cv=5, n_iter=8, random_state=20)\n",
    "\n",
    "random_grid.fit(X_train,y_train)\n",
    "\n",
    "print(\"the best R^2 of all model parameters' combination on SVR model: {:.2f}\".format(random_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the parameter setting of optimized SVR model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('svr', SVR(C=8.9153072947470804, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma=8.1583747730768401, kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the argument setting for best estimator\n",
    "print(\"the parameter setting of optimized SVR model\")\n",
    "random_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step10\"></a>\n",
    "## step10: optimized SVR model's performance on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the R^2 of the optimized SVR model on testing dataset:\n",
      "0.4127\n"
     ]
    }
   ],
   "source": [
    "# the estimator's performance on the testing dataset\n",
    "print(\"the R^2 of the optimized SVR model on testing dataset:\")\n",
    "print(\"{:.4f}\".format(random_grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step11\"></a>\n",
    "## step11: choose better model and predict the rating for the testing dataset\n",
    "As I use **R^2** as the criterion to decide which model(*Lasso or SVR*) performs better, it turns out **SVR** has a higher R^2 by a relatively small margin. I will use SVR as the final rating model and predict the **transformed_score** for the testing dataset, which I can compare with the true scores in later discussion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 281.75857095,  392.40150371,  554.86369912,  551.22471957,\n",
       "         90.05032052])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use SVR to predict the transformed_score in testing dataset\n",
    "svr_predict_transformed_score_test = random_grid.predict(X_test)\n",
    "\n",
    "# have a look at the predicted transformed_score\n",
    "svr_predict_transformed_score_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step12\"></a>\n",
    "## step12: save the output dataset for later use\n",
    "The datasets I will use for later discussion include:\n",
    "1. `X_remaining_sub` and `target_variable`: for the task in model evaluation and validation\n",
    "2. `X_test` and `y_test` and `svr_predict_transformed_score_test`: for examining the SVR model performance and comparing the `transformed_score`, `svr_predict_transformed_score_test` and original `Reviewer_Score`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the output dataset for later use\n",
    "X_remaining_sub.to_pickle(\"X_remaining_sub.pickle\")\n",
    "target_variable.to_pickle(\"target_variable.pickle\")\n",
    "X_test.to_pickle(\"X_test.pickle\")\n",
    "y_test.to_pickle(\"y_test.pickle\")\n",
    "pd.Series(svr_predict_transformed_score_test).to_pickle(\"svr_predict_transformed_score_test.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step13\"></a>\n",
    "## step13: appendix - increase the sample number in training dataset and re-fit Lasso model again\n",
    "1. It turns out that even I try to use a larger dataset for training Lasso model, the R^2 still under-perform than SVR model, which, by the way, use far less samples for model training.\n",
    "2. Nevertheless, both model seem not performing well in the starndard of originally set-up benchmark. \n",
    "3. The Lasso model only got **0.38 R^2** and SVR model only got **0.41 R^2**. Both are lower than **0.5 R^2** benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# increase samples in training dataset - model training version 2\n",
    "# separate target variable out - transformed_score\n",
    "target_variable_v2 = X_remaining_raw.transformed_score\n",
    "\n",
    "# drop out the target variable in X dataset\n",
    "X_remaining_sub_v2 = X_remaining_raw.drop([\"transformed_score\"], axis=1)\n",
    "\n",
    "# use the remaining 95% as the testing dataset \n",
    "X_train_v2, X_test_v2, y_train_v2, y_test_v2 = train_test_split(X_remaining_sub_v2, target_variable_v2,\n",
    "                                                                test_size = 0.25, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best R^2 of all model parameters' combination on Lasso model: 0.3898\n"
     ]
    }
   ],
   "source": [
    "# create a grid-searched Lasso model - version 2\n",
    "lasso = Lasso()\n",
    "\n",
    "# set up the parameter range for grid-search\n",
    "param_grid = {\"alpha\":[80,50,20,15,10,5,3,2,1,0.5,0.1,0.001],\n",
    "              \"max_iter\":[10000,5000,1000,500,100]}\n",
    "\n",
    "scorer = make_scorer(r2_score, greater_is_better=True)\n",
    "\n",
    "grid = GridSearchCV(lasso, param_grid=param_grid, scoring=scorer, cv=5)\n",
    "\n",
    "grid.fit(X_train_v2,y_train_v2)\n",
    "\n",
    "print(\"the best R^2 of all model parameters' combination on Lasso model: {:.4f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the parameter setting of optimized Lasso model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=10000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the argument setting for best estimator - version 2\n",
    "print(\"the parameter setting of optimized Lasso model\")\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the R^2 of the optimized Lasso model on testing dataset:\n",
      "0.3907\n"
     ]
    }
   ],
   "source": [
    "# the estimator's performance on the testing dataset - version 2\n",
    "print(\"the R^2 of the optimized Lasso model on testing dataset:\")\n",
    "print(\"{:.4f}\".format(grid.score(X_test_v2, y_test_v2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step14\"></a>\n",
    "## step14: appendix - try `DecisionTreeRegressor` for rate prediction model\n",
    "**REF:**<br/>\n",
    "[DecisionTreeRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate target variable out - transformed_score\n",
    "target_variable = X_remaining_raw.transformed_score\n",
    "\n",
    "# drop out the target variable in X dataset\n",
    "X_remaining_sub = X_remaining_raw.drop([\"transformed_score\"], axis=1)\n",
    "\n",
    "# use the remaining 95% as the testing dataset \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_remaining_sub, target_variable,\n",
    "                                                    test_size = 0.95, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best R^2 of all model parameters' combination on DecisionTreeRegressor model: 0.3782\n"
     ]
    }
   ],
   "source": [
    "# create a DecisionTreeRegressor model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "DecisionTR = DecisionTreeRegressor()\n",
    "\n",
    "# set up the parameter range for grid-search\n",
    "param_grid = {\"max_depth\":[50,20,10,5,3,2,1],\n",
    "              \"random_state\":[10]} # other options include: max_features, min_samples_leaf, max_leaf_nodes\n",
    "\n",
    "scorer = make_scorer(r2_score, greater_is_better=True)\n",
    "\n",
    "grid = GridSearchCV(DecisionTR, param_grid=param_grid, scoring=scorer, cv=5)\n",
    "\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "print(\"the best R^2 of all model parameters' combination on DecisionTreeRegressor model: {:.4f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the parameter setting of optimized DecisionTreeRegressor model\n",
      "DecisionTreeRegressor(criterion='mse', max_depth=5, max_features=None,\n",
      "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "           min_impurity_split=None, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           presort=False, random_state=10, splitter='best')\n",
      "\n",
      "\n",
      "the R^2 of the optimized DecisionTreeRegressor model on testing dataset:\n",
      "0.3740\n"
     ]
    }
   ],
   "source": [
    "# the argument setting for best estimator\n",
    "print(\"the parameter setting of optimized DecisionTreeRegressor model\")\n",
    "print(grid.best_estimator_)\n",
    "print(\"\\n\")\n",
    "\n",
    "# the estimator's performance on the testing dataset\n",
    "print(\"the R^2 of the optimized DecisionTreeRegressor model on testing dataset:\")\n",
    "print(\"{:.4f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step15\"></a>\n",
    "## step15: appendix - try `AdaBoosting` for rate prediction model\n",
    "**REF:**<br/>\n",
    "[AdaBoostRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html)<br/>\n",
    "[Decision Tree Regression with AdaBoost](http://scikit-learn.org/stable/auto_examples/ensemble/plot_adaboost_regression.html#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py)<br/>\n",
    "[Adaboost Classifier](https://chrisalbon.com/machine_learning/trees_and_forests/adaboost_classifier/)<br/>\n",
    "[Ensemble Machine Learning Algorithms in Python with scikit-learn](https://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best R^2 of all model parameters' combination on AdaBoostRegressor model: 0.3704\n"
     ]
    }
   ],
   "source": [
    "# create a DecisionTreeRegressor model\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "AdaBoost = AdaBoostRegressor()\n",
    "\n",
    "# set up the parameter range for grid-search\n",
    "param_grid = {\"n_estimators\":[100,80,50,20,10,5,3,2,1],\n",
    "              \"learning_rate\":[0.5,0.3,0.2,0.1,0.05,0.01],\n",
    "              \"random_state\":[10]}\n",
    "\n",
    "scorer = make_scorer(r2_score, greater_is_better=True)\n",
    "\n",
    "grid = GridSearchCV(AdaBoost, param_grid=param_grid, scoring=scorer, cv=5)\n",
    "\n",
    "grid.fit(X_train,y_train)\n",
    "\n",
    "print(\"the best R^2 of all model parameters' combination on AdaBoostRegressor model: {:.4f}\".format(grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the parameter setting of optimized AdaBoostRegressor model\n",
      "AdaBoostRegressor(base_estimator=None, learning_rate=0.2, loss='linear',\n",
      "         n_estimators=20, random_state=10)\n",
      "\n",
      "\n",
      "the R^2 of the optimized AdaBoostRegressor model on testing dataset:\n",
      "0.3670\n"
     ]
    }
   ],
   "source": [
    "# the argument setting for best estimator\n",
    "print(\"the parameter setting of optimized AdaBoostRegressor model\")\n",
    "print(grid.best_estimator_)\n",
    "print(\"\\n\")\n",
    "\n",
    "# the estimator's performance on the testing dataset\n",
    "print(\"the R^2 of the optimized AdaBoostRegressor model on testing dataset:\")\n",
    "print(\"{:.4f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step16\"></a>\n",
    "## step16: appendix - try `multi-layer perceptron` for rate prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils # encode categorical variable\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REF:[custom R^2 score for keras](https://www.kaggle.com/c/mercedes-benz-greener-manufacturing/discussion/34019) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom R2-score metrics for keras backend\n",
    "from keras import backend as K\n",
    "\n",
    "def r2_keras(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               4096      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 80,129\n",
      "Trainable params: 80,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model using categorical_crossentropy loss, and rmsprop optimizer.\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['mae',r2_keras])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# normalize input data\n",
    "mean = X_train.mean(axis=0)\n",
    "X_train = X_train - mean\n",
    "std = X_train.std(axis=0)\n",
    "X_train = X_train/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X_test - mean\n",
    "X_test = X_test/std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8796 samples, validate on 2932 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 21623.96487, saving model to rate.model.best.hdf5\n",
      "1s - loss: 44850.7644 - mean_absolute_error: 164.9602 - r2_keras: -3.8778e-01 - val_loss: 21623.9649 - val_mean_absolute_error: 119.9491 - val_r2_keras: 0.3093\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 21623.96487 to 20046.13426, saving model to rate.model.best.hdf5\n",
      "1s - loss: 21537.4712 - mean_absolute_error: 119.9884 - r2_keras: 0.3451 - val_loss: 20046.1343 - val_mean_absolute_error: 116.8699 - val_r2_keras: 0.3574\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 20046.13426 to 19074.28822, saving model to rate.model.best.hdf5\n",
      "1s - loss: 20558.0097 - mean_absolute_error: 116.4715 - r2_keras: 0.3746 - val_loss: 19074.2882 - val_mean_absolute_error: 113.4209 - val_r2_keras: 0.3909\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "0s - loss: 20344.6784 - mean_absolute_error: 115.8818 - r2_keras: 0.3865 - val_loss: 19285.6775 - val_mean_absolute_error: 116.2771 - val_r2_keras: 0.3847\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "0s - loss: 20139.2609 - mean_absolute_error: 114.9282 - r2_keras: 0.3913 - val_loss: 19343.6485 - val_mean_absolute_error: 112.5561 - val_r2_keras: 0.3835\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "0s - loss: 20082.9943 - mean_absolute_error: 114.8177 - r2_keras: 0.3918 - val_loss: 20111.7924 - val_mean_absolute_error: 113.5873 - val_r2_keras: 0.3597\n"
     ]
    }
   ],
   "source": [
    "# Running and evaluating the model\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='rate.model.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "earlystop = EarlyStopping(patience=2)\n",
    "\n",
    "hist = model.fit(X_train.as_matrix(), y_train.as_matrix(),\n",
    "          batch_size=50,\n",
    "          epochs=20,\n",
    "          validation_split=0.25,\n",
    "          callbacks=[checkpointer, earlystop],\n",
    "          verbose=2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make prediction\n",
    "mlp_rate_predict = model.predict(X_test.as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compare the predict value to the actual value\n",
    "from scipy.special import inv_boxcox\n",
    "inv_mlp_rate_predict = inv_boxcox(mlp_rate_predict, 3.3)\n",
    "inv_y_test = inv_boxcox(y_test.as_matrix(),3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.33035088],\n",
       "       [ 8.4790411 ],\n",
       "       [ 9.44911098],\n",
       "       [ 8.59064198],\n",
       "       [ 8.73257637]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_mlp_rate_predict[25:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.50265102,   4.6011632 ,  10.00412818,   6.70221246,   7.50265102])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_y_test[25:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# footnotes\n",
    "# 在SVM(在regression的案例中，使用SVR)\n",
    "# 在SVM中，要求每個feature最好有同樣的scale，所以需要進行標準化，常用的方式是將值標準化為在0到1之間(MinMaxScaler)\n",
    "# 在SVM中，要提高features的維度，主要有兩種方法：polynomial kernel，或是使用radial basis function(RBF) kernel\n",
    "# 可以fine tune的參數則是C與gamma\n",
    "# C就像是ridge function或是lasso function中討論的L2與L1的regularization\n",
    "# gamma指的是每一個資料點，所影響的範圍，如果gamma越小，則每個點影響範圍越廣，比較gerneralization，趨向underfitting\n",
    "# C則是越大的話，則模型會趨向複雜，overfitting\n",
    "# 在SVM裡面，也可以使用dummy variables\n",
    "\n",
    "# 在ridge function中使用的是alpha參數，當alpha越大時，coefficient會更趨近於0，相反的越小的alpha，則模型更複雜，趨向overfitting\n",
    "# 而在lasso function中，則需要注意到，當我們將alpha調小時，也需要同時增加max_iter(the maximum number of iterations)的參數才行, \n",
    "# 並且在lasso模型中，將alpha調小，是往overfitting的方向趨近\n",
    "# 一般來說，實務上第一個會嘗試的是ridge function，除非是features數量很多，需要削減，才會使用lasso function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
