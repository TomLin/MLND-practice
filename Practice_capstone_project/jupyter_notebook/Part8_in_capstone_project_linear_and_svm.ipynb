{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 在SVM(在regression的案例中，使用SVR)\n",
    "# 在SVM中，要求每個feature最好有同樣的scale，所以需要進行標準化，常用的方式是將值標準化為在0到1之間(MinMaxScaler)\n",
    "# 在SVM中，要提高features的維度，主要有兩種方法：polynomial kernel，或是使用radial basis function(RBF) kernel\n",
    "# 可以fine tune的參數則是C與gamma\n",
    "# C就像是ridge function或是lasso function中討論的L2與L1的regularization\n",
    "# gamma指的是每一個資料點，所影響的範圍，如果gamma越小，則每個點影響範圍越廣，比較gerneralization，趨向underfitting\n",
    "# C則是越大的話，則模型會趨向複雜，overfitting\n",
    "# 在SVM裡面，也可以使用dummy variables\n",
    "\n",
    "# 在ridge function中使用的是alpha參數，當alpha越大時，coefficient會更趨近於0，相反的越小的alpha，則模型更複雜，趨向overfitting\n",
    "# 而在lasso function中，則需要注意到，當我們將alpha調小時，也需要同時增加max_iter(the maximum number of iterations)的參數才行, \n",
    "# 並且在lasso模型中，將alpha調小，是往overfitting的方向趨近\n",
    "# 一般來說，實務上第一個會嘗試的是ridge function，除非是features數量很多，需要削減，才會使用lasso function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [step0](#step0): import necessary packages\n",
    "* [step1](#step1): import `dataset X_remaining50.pickle` as `X_remaining50`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno # module for missing value visualization\n",
    "from scipy import stats # implement box-cox transformation\n",
    "from math import ceil\n",
    "from sklearn.utils import shuffle # shuffling the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB # for sentiment analysis benchmark model\n",
    "from sklearn.model_selection import cross_val_score # cross validation score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "from scipy.stats import uniform\n",
    "from numpy import flatnonzero # return the index for nonzero value\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = None # show up all column values in display\n",
    "\n",
    "# suppress warning\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# suppress scientific notation\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step1\"></a>\n",
    "## step1: import dataset `X_remaining50.pickle` as `X_remaining50`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_remaining50 = pd.read_pickle(\"X_remaining50.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step2\"></a>\n",
    "## step2: select the relevant predictors\n",
    "1. the target variable is `transformed_score`.\n",
    "2. based on the correlation matrix showed up in part3 and part4, `Review_Total_Negative_Word_Counts`, `Review_Total_Positive_Word_Counts`, `quarter_transformed_score`, `quarter_previous_transformed_score` have better correlation with `transformed_score`.\n",
    "3. in order to apply pd.get_dummies(), I first replace the value 0/1 in `bayes_predict_review_sentiment` to string.\n",
    "4. drop out the NA in the rows, so that I won't face bugs when applying MinMaxScaler()\n",
    "5. set up the dummies for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select the relevant predictors\n",
    "cols = [\"transformed_score\", \"Average_Score\",\n",
    "        \"Review_Total_Negative_Word_Counts\",\"Review_Total_Positive_Word_Counts\",\n",
    "        \"quarter_transformed_score\",\"quarter_previous_transformed_score\",\n",
    "        \"quarter_change_rate\",\"bayes_predict_review_sentiment\"]\n",
    "\n",
    "X_remaining_sub = X_remaining50[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# map 0/1 in bayes_predict_review_sentiment to negative/positive sentiment\n",
    "value_replace = {0:\"negative\",\n",
    "                 1:\"positive\"}\n",
    "\n",
    "X_remaining_sub[\"bayes_predict_review_sentiment\"] = X_remaining_sub[\"bayes_predict_review_sentiment\"].map(value_replace)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop out row contains NA\n",
    "X_remaining_sub.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert categorical feature into dummies\n",
    "X_remaining_sub = pd.get_dummies(X_remaining_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step3\"></a>\n",
    "## step3: shuffle and sampling  the remaining dataset\n",
    "Use the remaining 2.5% of dataset as training and validation dataset for rating prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate target variable out - transformed_score\n",
    "target_variable = X_remaining_sub.transformed_score\n",
    "\n",
    "# drop out the target variable in X dataset\n",
    "X_train = X_remaining_sub.drop([\"transformed_score\"], axis=1)\n",
    "\n",
    "# use the remaining 50% of the whole dataset - use train_test_split() to achieve same result\n",
    "X_first, X_remaining, y_first, y_remaining = train_test_split(X_train, target_variable,\n",
    "                                                              test_size = 0.95, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step4\"></a>\n",
    "## step4: create lasso model as benchmark model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39510146385041484\n"
     ]
    }
   ],
   "source": [
    "# create a Lasso model\n",
    "lasso = Lasso()\n",
    "\n",
    "# set up the parameter range for grid-search\n",
    "param_grid = {\"alpha\":[80,50,20,15,10,5,3,2,1,0.5,0.1,0.001],\n",
    "              \"max_iter\":[10000,5000,1000,500,100]}\n",
    "\n",
    "scorer = make_scorer(r2_score, greater_is_better=True)\n",
    "\n",
    "grid = GridSearchCV(lasso, param_grid=param_grid, scoring=scorer, cv=5)\n",
    "\n",
    "grid.fit(X_first,y_first)\n",
    "\n",
    "print(grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.1, copy_X=True, fit_intercept=True, max_iter=10000,\n",
       "   normalize=False, positive=False, precompute=False, random_state=None,\n",
       "   selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the argument setting for best estimator\n",
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3920149477267595"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the estimator's performance on the remaining 40% dataset\n",
    "grid.score(X_remaining40, y_remaining40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filter out the nonzero coefficients\n",
    "nonzero_index = flatnonzero(grid.best_estimator_.coef_) # return the index for nonzero coefficients\n",
    "nonzero_feature_name = X_first10.columns[nonzero_index] # index out the feature name\n",
    "nonzero_coef = grid.best_estimator_.coef_[nonzero_index] # index out the coefficient's value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Review_Total_Positive_Word_Counts</td>\n",
       "      <td>1.406859e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>quarter_transformed_score</td>\n",
       "      <td>7.435925e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bayes_predict_review_sentiment_positive</td>\n",
       "      <td>9.515553e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quarter_previous_transformed_score</td>\n",
       "      <td>-2.598417e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review_Total_Negative_Word_Counts</td>\n",
       "      <td>-1.475442e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>quarter_change_rate</td>\n",
       "      <td>-1.595652e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>bayes_predict_review_sentiment_negative</td>\n",
       "      <td>-1.430481e+02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   feature         value\n",
       "0        Review_Total_Positive_Word_Counts  1.406859e+00\n",
       "1                quarter_transformed_score  7.435925e-01\n",
       "2  bayes_predict_review_sentiment_positive  9.515553e-13\n",
       "3       quarter_previous_transformed_score -2.598417e-02\n",
       "4        Review_Total_Negative_Word_Counts -1.475442e+00\n",
       "5                      quarter_change_rate -1.595652e+00\n",
       "6  bayes_predict_review_sentiment_negative -1.430481e+02"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the coefficients as sorted dataframe\n",
    "value = nonzero_coef[nonzero_coef.argsort()[::-1]]\n",
    "feature = nonzero_feature_name[nonzero_coef.argsort()[::-1]]\n",
    "lasso_coefficient = pd.DataFrame({\"feature\":feature, \"value\":value})\n",
    "display(lasso_coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step5\"></a>\n",
    "## step5: create support vector regression model\n",
    "1. later I will use Support Vector Machine, it requires to have smimilar scale on all features. For I have dummy variables(0/1), I will use MinMaxScaler() to scale all numeric features into range 0/1 as well. \n",
    "2. in the following model evaluation, for the proper use of train and validation dataset in corss-validation, it's better to create a pipeline for it.\n",
    "3. for each fold of cross-validation, pipeline enables to use the training data within current fold only to create scaler. It avoids data information leakage to validation dataset.\n",
    "4. for support vector machine, the observations of dataset highly recommended not to go beyond 100,000 rows, the time it takes for training the model goes exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4163486677776579\n"
     ]
    }
   ],
   "source": [
    "# create a randomized pipeline\n",
    "svr_pipe = Pipeline([(\"scaler\", MinMaxScaler()),(\"svr\", SVR())])\n",
    "\n",
    "\n",
    "# set up the parameter range for grid-search\n",
    "param_grid = {\"svr__C\":uniform(0,10), # use distributions insted (only applicable in randomized grid search)\n",
    "              \"svr__gamma\":uniform(0,10)}\n",
    "\n",
    "scorer = make_scorer(r2_score, greater_is_better=True)\n",
    "\n",
    "random_grid = RandomizedSearchCV(svr_pipe, param_distributions=param_grid, # use param_distributions\n",
    "                                 scoring=scorer, cv=3, n_iter=5, random_state=20)\n",
    "random_grid.fit(X_first,y_first)\n",
    "\n",
    "print(random_grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('svr', SVR(C=8.91530729474708, cache_size=200, coef0=0.0, degree=3, epsilon=0.1,\n",
       "  gamma=8.15837477307684, kernel='rbf', max_iter=-1, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41633340843065847"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_grid.score(X_remaining, y_remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## another version for normalizing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/TomLin/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/TomLin/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:11: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# scale all variables including target variable\n",
    "scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "num_cols = [\"Review_Total_Negative_Word_Counts\",\"Review_Total_Positive_Word_Counts\",\n",
    "            \"quarter_transformed_score\",\"quarter_previous_transformed_score\",\n",
    "            \"quarter_change_rate\"]\n",
    "\n",
    "X_first30[num_cols] = scaler.fit_transform(X_first30[num_cols])\n",
    "\n",
    "y_first30 = target_scaler.fit_transform(y_first30.reshape(-1,1))\n",
    "\n",
    "y_first30 = y_first30.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step6\"></a>\n",
    "## step6: convert the transformed_score back to original scale\n",
    "1. have a look at the coefficient on each feature, to grip a sense of how the features influence the response.\n",
    "2. convert the transfomred_socre back to original scale and see have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
