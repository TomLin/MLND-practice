{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of steps\n",
    "* [step0](#step0): import necessary packages\n",
    "* [step1](#step1): import dataset `part6_dataset.pickle` as `part7_dataset`\n",
    "* [step2](#step2): stratified sampling **50%** of the dataset\n",
    "* [step3](#step3): create necessary `class` for sentiment analysis\n",
    "* [step4](#step4): create a bag of words solely for sentiment analysis\n",
    "* [step5](#step5): build up the benchmark model (naive bayes) for sentiment analysis\n",
    "* [step6](#step6): evaluate the naive bayes model performance based on cross-validation\n",
    "* [step7](#step7): general output of the model prediction and confusion matrix on the training dataset (nb model)\n",
    "* [step8](#step8): evaluate navie bayes model on the `testing` dataset\n",
    "* [step9](#step9): the comparison model - multi-layer perceptron model\n",
    "* [step10](#step10): general output of the model prediction and confusion matrix on the training dataset (mlp model)\n",
    "* [step11](#step11): evaluate multi-layer perceptron model on the `testing` dataset\n",
    "* [step12](#step12): choose the better model and predict text sentiment for the remaining 50% dataset\n",
    "* [step13](#step13): export the remaining 50% dataset for the next modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno # module for missing value visualization\n",
    "from scipy import stats # implement box-cox transformation\n",
    "from math import ceil\n",
    "from sklearn.utils import shuffle # shuffling the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB # for sentiment analysis benchmark model\n",
    "from sklearn.model_selection import cross_val_score # cross validation score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from keras.utils import np_utils # encode categorical variable\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping  \n",
    "\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step1\"></a>\n",
    "## step1: import dataset `part6_dataset.pickle` as `part7_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part7_dataset = pd.read_pickle(\"part6_dataset.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step2\"></a>\n",
    "## step2: stratified sampling 50% of the dataset\n",
    "1. Use the **first 50%** of dataset as training and validation dataset for sentiment analysis, which can be achieved by using train_test_split() to achieve same result.\n",
    "    - a. Especially, for the first 50% of dataset, I will split out 10% again for use as testing dataset, so that I can better evaluate the performance between Naive Bayes and Multi-layer Perceptron model.\n",
    "    - b. This manner also avoids **data leakage**, I don't use the remaining 50% of the population dataset for guiding what model should be chosen.\n",
    "    - c. All the practices are implemented by the function **train_test_split**, and **classes are stratified** as well.\n",
    "2. The **remaining 50%** can be used later for rating prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate target variable out - review_sentiment\n",
    "target_variable = part7_dataset.review_sentiment\n",
    "target_variable = target_variable.astype(\"category\")\n",
    "\n",
    "# just sample 50% of the whole dataset - use train_test_split() to achieve same result\n",
    "X_first50, X_remaining50, y_first50, y_remaining50 = train_test_split(part7_dataset, target_variable,\n",
    "                                                                      test_size = 0.5, stratify = target_variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# separate out a 10% testing dataset to evaluate the performance between Naive Bayes and Multi-layer Perceptron\n",
    "X_first40, X_test, y_first40, y_test = train_test_split(X_first50, y_first50,\n",
    "                                                        test_size = 0.2, stratify = y_first50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step3\"></a>\n",
    "## step3: create necessary `class` for sentiment analysis\n",
    "1. In order to come up with better text sentiment model, I further implement advanced lemmatization to clean up the text data. It is referred from [sklearn customized lemmatizer](http://scikit-learn.org/stable/modules/feature_extraction.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a class for lemmatizer\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step4\"></a>\n",
    "## step4: create a bag of words solely for sentiment analysis\n",
    "1. Setting the ngram up to **bi-grams**, so that we can further experiment to see if potential influential word patterns exist. \n",
    "2. Owing to later on I will use **MultinomialNB model**, so it's better to use **`CountVectorizer`** instead of `TfidfVectorizer` in this bag of word model, **contrary to the `TfidfVectorizer` being used in LDA model**.\n",
    "3. Again, words to be collected in bag of words model is set **5000**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build up a bag of words for Sentiment Analysis\n",
    "n_features = 5000\n",
    "\n",
    "sentiment_count_vectorizer = CountVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                                             max_df=0.5, min_df=2, # word fequency less than 50% and shows at least in 2 doc\n",
    "                                             max_features=n_features,\n",
    "                                             stop_words=\"english\",\n",
    "                                             ngram_range=(1,2))\n",
    "\n",
    "# fit and transform data\n",
    "sentiment_count = sentiment_count_vectorizer.fit_transform(X_first40[\"combined_review\"])\n",
    "sentiment_count = sentiment_count.toarray() # transform from sparse to dense matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step5\"></a>\n",
    "## step5: build up the benchmark model (naive bayes) for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the benchmark model\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(sentiment_count, y_first40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step6\"></a>\n",
    "## step6: evaluate the naive bayes model performance based on cross-validation \n",
    "1. In this classification case, the metrics include **accuracy**, **f1**, **recall**. \n",
    "    - a. In hindsight, the **accuracy** for this text sentiment model should be greater than **0.89**, because the probability for the positive review class is 0.89 and probability for the negative review class is (1-0.89).\n",
    "    - b. Hence, ideally it's better to have a model with accuracy higher than 0.89. \n",
    "2. Cross-validation fold is set to be **5**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean accuracy of corss-validation: 0.86\n",
      "mean f1 of corss-validation: 0.92\n",
      "mean recall of corss-validation: 0.88\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model performance based on cross-validation\n",
    "mean_accuracy = cross_val_score(estimator=naive_bayes, X=sentiment_count, y=y_first40, cv=5, scoring=\"accuracy\").mean()\n",
    "print(\"mean accuracy of corss-validation: {}\".format(round(mean_accuracy,2)))\n",
    "\n",
    "# calculate the f1 score\n",
    "mean_f1 = cross_val_score(estimator=naive_bayes, X=sentiment_count, y=y_first40, cv=5, scoring=\"f1\").mean()\n",
    "print(\"mean f1 of corss-validation: {}\".format(round(mean_f1,2)))\n",
    "\n",
    "# calculate the recall\n",
    "mean_recall = cross_val_score(estimator=naive_bayes, X=sentiment_count, y=y_first40, cv=5, scoring=\"recall\").mean()\n",
    "print(\"mean recall of corss-validation: {}\".format(round(mean_recall,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step7\"></a>\n",
    "## step7: general output of the model prediction and confusion matrix on the training dataset (nb model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of training dataset\n",
      "0.87\n",
      "\n",
      "\n",
      "confusion matrix from naive bayes model\n",
      "[[ 14738   6197]\n",
      " [ 21189 162653]]\n",
      "\n",
      "\n",
      "the overview of performance metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.41      0.70      0.52     20935\n",
      "          1       0.96      0.88      0.92    183842\n",
      "\n",
      "avg / total       0.91      0.87      0.88    204777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create the prediction from naive_bayes model\n",
    "prediction = naive_bayes.predict(sentiment_count)\n",
    "\n",
    "# print the accuracy metric\n",
    "print(\"the accuracy of training dataset\")\n",
    "print(round(accuracy_score(y_first40, prediction),2))\n",
    "print(\"\\n\")\n",
    "\n",
    "# print the confusion matrix\n",
    "print(\"confusion matrix from naive bayes model\")\n",
    "print(confusion_matrix(y_first40, prediction))\n",
    "print(\"\\n\")\n",
    "\n",
    "# print the overview of performance metrics\n",
    "print(\"the overview of performance metrics\")\n",
    "print(classification_report(y_first40, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step8\"></a>\n",
    "## step8: evaluate navie bayes model on the `testing` dataset.\n",
    "**REMINDING**: the testing dataset for text sentiment model is `X_test` and `y_test`, not `X_remaining50` and `y_remaining50`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform the testing dataset\n",
    "test_sentiment_count = sentiment_count_vectorizer.transform(X_test[\"combined_review\"])\n",
    "test_sentiment_count = test_sentiment_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of testing dataset\n",
      "0.86\n",
      "\n",
      "\n",
      "confusion matrix from naive bayes model (in remaining test dataset)\n",
      "[[ 3629  1605]\n",
      " [ 5351 40610]]\n",
      "\n",
      "\n",
      "the overview of performance metrics (in remaining test dataset)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.40      0.69      0.51      5234\n",
      "          1       0.96      0.88      0.92     45961\n",
      "\n",
      "avg / total       0.90      0.86      0.88     51195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the performance on the testing dataset\n",
    "test_prediction = naive_bayes.predict(test_sentiment_count)\n",
    "\n",
    "# print the accuracy metric\n",
    "print(\"the accuracy of testing dataset\")\n",
    "print(round(accuracy_score(y_test, test_prediction),2))\n",
    "print(\"\\n\")\n",
    "\n",
    "# print the confusion matrix\n",
    "print(\"confusion matrix from naive bayes model (in remaining test dataset)\")\n",
    "print(confusion_matrix(y_test, test_prediction))\n",
    "print(\"\\n\")\n",
    "\n",
    "# print the overview of performance metrics\n",
    "print(\"the overview of performance metrics (in remaining test dataset)\")\n",
    "print(classification_report(y_test, test_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step9\"></a>\n",
    "## step9: the comparison model - multi-layer perceptron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode the target variable\n",
    "target_variable = np_utils.to_categorical(y_first40, num_classes=2) # training\n",
    "test_target_varialbe = np_utils.to_categorical(y_test, num_classes=2) # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_43 (Dense)             (None, 512)               2560512   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 2,636,578\n",
      "Trainable params: 2,636,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Building the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(n_features,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "# Compiling the model using categorical_crossentropy loss, and rmsprop optimizer.\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 153582 samples, validate on 51195 samples\n",
      "Epoch 1/20\n",
      "Epoch 00000: val_loss improved from inf to 0.22081, saving model to sentiment.model.best.hdf5\n",
      "27s - loss: 0.2327 - acc: 0.9126 - val_loss: 0.2208 - val_acc: 0.9162\n",
      "Epoch 2/20\n",
      "Epoch 00001: val_loss improved from 0.22081 to 0.21949, saving model to sentiment.model.best.hdf5\n",
      "26s - loss: 0.2167 - acc: 0.9203 - val_loss: 0.2195 - val_acc: 0.9170\n",
      "Epoch 3/20\n",
      "Epoch 00002: val_loss improved from 0.21949 to 0.21876, saving model to sentiment.model.best.hdf5\n",
      "26s - loss: 0.2121 - acc: 0.9229 - val_loss: 0.2188 - val_acc: 0.9178\n",
      "Epoch 4/20\n",
      "Epoch 00003: val_loss did not improve\n",
      "26s - loss: 0.2084 - acc: 0.9257 - val_loss: 0.2240 - val_acc: 0.9174\n",
      "Epoch 5/20\n",
      "Epoch 00004: val_loss did not improve\n",
      "26s - loss: 0.2045 - acc: 0.9287 - val_loss: 0.2293 - val_acc: 0.9172\n",
      "Epoch 6/20\n",
      "Epoch 00005: val_loss did not improve\n",
      "26s - loss: 0.1984 - acc: 0.9321 - val_loss: 0.2358 - val_acc: 0.9163\n"
     ]
    }
   ],
   "source": [
    "# Running and evaluating the model\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='sentiment.model.best.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "earlystop = EarlyStopping(patience=2)\n",
    "\n",
    "hist = model.fit(sentiment_count, target_variable,\n",
    "          batch_size=50,\n",
    "          epochs=20,\n",
    "          validation_split=0.25,\n",
    "          callbacks=[checkpointer, earlystop],\n",
    "          verbose=2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step10\"></a>\n",
    "## step10: general output of the model prediction and confusion matrix on the training dataset (mlp model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of training dataset\n",
      "0.93\n",
      "\n",
      "\n",
      "confusion matrix from multi-layer perceptron model\n",
      "[[  9229  11706]\n",
      " [  2028 181814]]\n",
      "\n",
      "\n",
      "the overview of performance metrics\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.44      0.57     20935\n",
      "          1       0.94      0.99      0.96    183842\n",
      "\n",
      "avg / total       0.93      0.93      0.92    204777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create the prediction from multi-layer perceptron model\n",
    "prediction_mlp = model.predict(sentiment_count)\n",
    "prediction_mlp = prediction_mlp.argmax(axis=1)\n",
    "\n",
    "# print the accuracy metric\n",
    "print(\"the accuracy of training dataset\")\n",
    "print(round(accuracy_score(y_first40, prediction_mlp),2))\n",
    "print(\"\\n\")\n",
    "\n",
    "# print the confusion matrix\n",
    "print(\"confusion matrix from multi-layer perceptron model\")\n",
    "print(confusion_matrix(y_first40, prediction_mlp))\n",
    "print(\"\\n\")\n",
    "\n",
    "# print the overview of performance metrics\n",
    "print(\"the overview of performance metrics\")\n",
    "print(classification_report(y_first40, prediction_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step11\"></a>\n",
    "## step11: evaluate multi-layer perceptron model on the `testing` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy of testing dataset\n",
      "0.92\n",
      "\n",
      "\n",
      "confusion matrix from multi-layer perceptron model (in remaining test dataset)\n",
      "[[ 1852  3382]\n",
      " [  810 45151]]\n",
      "\n",
      "\n",
      "the overview of performance metrics (in remaining test dataset)\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.35      0.47      5234\n",
      "          1       0.93      0.98      0.96     45961\n",
      "\n",
      "avg / total       0.91      0.92      0.91     51195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate the performance on the testing dataset\n",
    "test_prediction_mlp = model.predict(test_sentiment_count)\n",
    "test_prediction_mlp = test_prediction_mlp.argmax(axis=1)\n",
    "\n",
    "# print the accuracy metric\n",
    "print(\"the accuracy of testing dataset\")\n",
    "print(round(accuracy_score(y_test, test_prediction_mlp),2))\n",
    "print(\"\\n\")\n",
    "\n",
    "# print the confusion matrix\n",
    "print(\"confusion matrix from multi-layer perceptron model (in remaining test dataset)\")\n",
    "print(confusion_matrix(y_test, test_prediction_mlp))\n",
    "print(\"\\n\")\n",
    "\n",
    "# print the overview of performance metrics\n",
    "print(\"the overview of performance metrics (in remaining test dataset)\")\n",
    "print(classification_report(y_test, test_prediction_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step12\"></a>\n",
    "## step12: choose the better model and predict text sentiment for the remaining 50% dataset\n",
    "The brief performance comparison between **Naive Bayes** and **Multi-layer Perceptron** model:\n",
    "\n",
    "| Naive Bayes \t|  Metric  \t| Multi-layer Perceptron \t|\n",
    "|------------:\t|:--------:\t|------------------------\t|\n",
    "|        0.86 \t| Accuracy \t| 0.92                   \t|\n",
    "|        0.88 \t| F1-score \t| 0.91                   \t|\n",
    "|        0.86 \t|  Recall  \t| 0.92                   \t|\n",
    "\n",
    "* class 0: negative review\n",
    "* class 1: positive review\n",
    "\n",
    "As we can tell, the MLP model seems to outperform the Naive Bayes model a little bit on all three metrics. But if we look closer, on the unbalanced(fewer) class 0, of all class 0, Naive Bayes can identify correctly 69% in testing dataset while the MLP model can only identify 35% in testing dataset. That means Naive Bayes model has a better performance of recall and f1 on the small class (that is the 0 group). If ientifying class 0 is very important, then Naive Bayes model should be chosen instead. \n",
    "<br/><br/>\n",
    "However, in my case, I will weight class 0 and 1 equally, and choose the model with higher overall performance - that is **Multi-layer Perceptron Model**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit and transform remaining data into bag of words\n",
    "remaining_sentiment_count = sentiment_count_vectorizer.fit_transform(X_remaining50[\"combined_review\"])\n",
    "remaining_sentiment_count = remaining_sentiment_count.toarray() # transform from sparse to dense matrix\n",
    "\n",
    "# predict the text sentiment for the remaining 50% dataset\n",
    "mlp_predict_review_sentiment = model.predict(remaining_sentiment_count)\n",
    "mlp_predict_review_sentiment = mlp_predict_review_sentiment.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step13\"></a>\n",
    "## step13: export the remaining 50% dataset for the next modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# add in the predicted review sentiment from MLP model \n",
    "X_remaining50[\"mlp_predict_review_sentiment\"] = mlp_predict_review_sentiment\n",
    "X_remaining50.to_pickle(\"X_remaining50.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
