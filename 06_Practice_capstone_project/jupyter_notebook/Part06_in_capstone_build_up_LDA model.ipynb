{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of steps\n",
    "* [step0](#step0): import necessary packages\n",
    "* [step1](#step1): import dataset `part5_dataset.pickle` as `part6_dataset`\n",
    "* [step2](#step2): extract `combined_review` from `part6_dataset`\n",
    "* [step3](#step3): create necessary `class` and `self-defined-fun` for LDA model\n",
    "* [step4](#step4): create a bag of words solely for LDA model\n",
    "* [step5](#step5): build up a LDA model\n",
    "* [step6](#step6): calculate the topic probabilities for each document\n",
    "* [step7](#step7): have a look at the doc topic assigned and the doc text\n",
    "* [step8](#step8): join the topic back to the original dataset\n",
    "* [step9](#step9): save the output as `part6_dataset.pickle`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno # module for missing value visualization\n",
    "from scipy import stats # implement box-cox transformation\n",
    "from math import ceil\n",
    "from string import strip # Return a copy of the string with leading and trailing characters removed\n",
    "from sklearn.utils import shuffle # shuffling the dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step1\"></a>\n",
    "## step1: import dataset part5_dataset.pickle as part6_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part6_dataset = pd.read_pickle(\"part5_dataset.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step2\"></a>\n",
    "## step2: extract `combined_review` from `part6_dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_review = part6_dataset[\"combined_review\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step3\"></a>\n",
    "## step3: create necessary `class` and `self-defined-fun` for LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a class for lemmatizer\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define a function to show up the topic words\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: \n",
    "1. [customized lemmatizer](http://scikit-learn.org/stable/modules/feature_extraction.html)\n",
    "2. [Topic extraction with Non-negative Matrix Factorization and Latent Dirichlet Allocation](http://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py)\n",
    "3. [extended slices](https://docs.python.org/2.3/whatsnew/section-slices.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step4\"></a>\n",
    "## step4: create a bag of words solely for LDA model\n",
    "There are some specifications for the **bag of words**:\n",
    "1. include all dataset\n",
    "2. using Term frequency - Inverse Document Frequency Tokenizer \n",
    "3. only pick up 5000 words\n",
    "4. use advanced Lemmatization\n",
    "5. only pick up word fequency less than 25% and shows at least in 2 documents\n",
    "6. exclude English stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build up a bag of words for LDA model\n",
    "n_features = 5000\n",
    "\n",
    "lda_tfidf_vectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(),\n",
    "                                       max_df=0.25, min_df=2, # word fequency less than 25% and shows at least in 2 doc\n",
    "                                       max_features=n_features,\n",
    "                                       stop_words=\"english\")\n",
    "\n",
    "# fit and transform data\n",
    "lda_tfidf = lda_tfidf_vectorizer.fit_transform(combined_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract the feature names in the bag of words\n",
    "lda_tfidf_feature_names = lda_tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step5\"></a>\n",
    "## step5: build up a LDA model\n",
    "Notice that in LDA model, we also need to set up the `random_state` for reproductive purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics in LDA model:\n",
      "Topic #0: t u check time reception day service stay like didn night breakfast positive did booking thing slow got booked bar\n",
      "Topic #1: great breakfast friendly good excellent helpful clean bed nice comfortable service perfect amazing lovely pool comfy small food fantastic facility\n",
      "Topic #2: positive bed bathroom breakfast shower good small coffee wifi water air t window poor hot noisy tea star old floor\n",
      "Topic #3: close station city good metro great walk central near clean nice breakfast easy center parking restaurant centre train far minute\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# build up a LDA model\n",
    "n_topics = 4\n",
    "n_top_words = 20\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "\n",
    "# fit in the data\n",
    "lda.fit(lda_tfidf)\n",
    "\n",
    "print(\"Topics in LDA model:\")\n",
    "lda_tfidf_feature_names = lda_tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, lda_tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Interpretation for the topics\n",
    "\n",
    "| Topic \t| Interpretation                                                                   \t| Top N words in the topic                                                                                                                           \t|\n",
    "|------:\t|:----------------------------------------------------------------------------------\t|:----------------------------------------------------------------------------------------------------------------------------------------------------\t|\n",
    "|     0 \t| (possibly negative) related to check, reception, booking, slow                   \t| check time reception day service stay like didn night breakfast positive did booking thing slow got booked bar                                     \t|\n",
    "|     1 \t| (quite positive) related to friendly, breakfast, clean, comfortable, service     \t| great breakfast friendly good excellent helpful clean bed nice comfortable service perfect amazing lovely pool comfy small food fantastic facility \t|\n",
    "|     2 \t| (possibly negative) related to bathroom, shower, small, coffee, wifi, noisy, old \t| bed bathroom breakfast shower good small coffee wifi water air t window poor hot noisy tea star old floor                                          \t|\n",
    "|     3 \t| (quite positive) related to close, station, metro, central, near, parking        \t| close station city good metro great walk central near clean nice breakfast easy center parking restaurant centre train far minute                  \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a dataframe of topic interpretation\n",
    "topic_interpretation = pd.DataFrame({\"topic\":[0,1,2,3],\n",
    "                                     \"topic_interpretation\":[\n",
    "                                                \"(possibly negative) related to check, reception, booking, slowÂ \",\n",
    "                                                \"(quite positive) related to friendly, breakfast, clean, comfortable, service\",\n",
    "                                                \"(possibly negative) related to bathroom, shower, small, coffee, wifi, noisy, old\",\n",
    "                                                \"(quite positive) related to close, station, metro, central, near, parking\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step6\"></a>\n",
    "## step6: calculate the topic probabilities for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the topic probabilities for each document(row)\n",
    "doc_topic_proba = lda.transform(lda_tfidf) # already being normalized and will sum up to 1\n",
    "\n",
    "# receive the index for topic with maximum probability\n",
    "doc_topic_max = doc_topic_proba.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: \n",
    "1. [LDA probability for document](https://github.com/scikit-learn/scikit-learn/issues/6320)\n",
    "2. [Use .transform for topic probatility](https://stackoverflow.com/questions/45150329/how-to-get-the-topics-probability-of-a-specific-document-using-scikit-learn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step\"></a>\n",
    "## step7: have a look at the doc topic assigned and the doc text\n",
    "See if the topic being assigned matches up the doc text review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nothing Lovely hotel with extremely comfortable huge double bed We stayed in the split level room which we really liked If you have difficulty getting up stairs request if you can stay in a room all on one level The Oosterpark is beautiful the shops and restaurants are great with lots of variety to choose from You can get the Metro close by 8min walk or the Tram is a short walk away and runs from the station and you can get off within a 5 mins walk to the Hotel All in all a beautiful hotel with friendly staff shampoo and soap in the shower Tea and coffee facilities in your room and in a location that is more relaxing than the central Amsterdam We will be returning'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the full length of doc text - use df.iloc[]\n",
    "combined_review.iloc[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the topic assigned to the doc\n",
    "doc_topic_max[24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step8\"></a>\n",
    "## step8: join the topic back to the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join the topic back to the original dataset\n",
    "part6_dataset[\"topic_assign_by_LDA\"] = doc_topic_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join the topic interpretation\n",
    "part6_dataset = pd.merge(part6_dataset, topic_interpretation,\n",
    "                         left_on=\"topic_assign_by_LDA\", right_on=\"topic\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# drop duplicated column\n",
    "part6_dataset.drop(\"topic\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step9\"></a>\n",
    "## step9: save the output as part6_dataset.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "part6_dataset.to_pickle(\"part6_dataset.pickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
